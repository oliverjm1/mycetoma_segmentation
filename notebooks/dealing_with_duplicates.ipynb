{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preprocessing Training Data\n","\n","The Ground Truth masks are a bit problematic, because often only one of multiple grains are included in the ground truth mask.\n","There are some events of the same image being present twice, with two different ground truth masks.\n","This notebook will look at finding these duplicate images and combining the masks into one.\n","\n","## To Do\n","1. Create list of patient ID (duplicate images will always belong to the same patient as eachother).\n","1. Search through images of each patient, checking for duplicates.\n","1. If there is a duplicate, combine the masks, overwrite the mask, and delete the second image/mask occurrence."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1654,"status":"ok","timestamp":1720776143607,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"uOgfpJNngHPy"},"outputs":[],"source":["# imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import glob\n","import sys\n","import hashlib\n","from PIL import Image\n","from collections import defaultdict\n","\n","sys.path.append('../src')\n","from utils import return_image_and_mask, return_image, return_mask, plot_image_and_mask"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":943,"status":"ok","timestamp":1720776292463,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"1HMxckWoOcN9","outputId":"b6c0de01-f80d-4ca3-9459-1e940bded25e"},"outputs":[{"name":"stdout","output_type":"stream","text":["data.md            \u001b[34mtraining_dataset\u001b[m\u001b[m   \u001b[34mvalidation_dataset\u001b[m\u001b[m\n","train.tar.gz       valid.tar.gz\n"]}],"source":["!ls ../data/"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1720776338533,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"2bbLeT3YOY4r"},"outputs":[],"source":["DATA_DIR = '../data'"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11894,"status":"ok","timestamp":1720776356840,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"gHPGNzTAH9hb","outputId":"10e59240-050d-4b9f-a243-cdecd21405b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train length: 593\n","Val length: 91\n"]}],"source":["# Get full image path by adding filename to base path\n","\n","# Get the paths\n","train_paths = np.array([os.path.relpath(i, DATA_DIR).split('.')[0] for i in glob.glob(f'{DATA_DIR}/training_dataset/**/*.jpg')])\n","val_paths = np.array([os.path.relpath(i, DATA_DIR).split('.')[0] for i in glob.glob(f'{DATA_DIR}/validation_dataset/**/*.jpg')])\n","\n","print(f\"Train length: {len(train_paths)}\")\n","print(f\"Val length: {len(val_paths)}\")"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Sort paths to make sorting by patient easier\n","train_paths.sort()\n","val_paths.sort()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['training_dataset/BM/BM32_1' 'training_dataset/BM/BM32_2'\n"," 'training_dataset/BM/BM32_3' 'training_dataset/BM/BM32_4'\n"," 'training_dataset/BM/BM32_5' 'training_dataset/BM/BM33_1'\n"," 'training_dataset/BM/BM35_1' 'training_dataset/BM/BM35_2'\n"," 'training_dataset/BM/BM35_3' 'training_dataset/BM/BM35_4']\n"]}],"source":["idx = 70\n","print(train_paths[idx:idx+10])"]},{"cell_type":"markdown","metadata":{},"source":["## Removing Duplicates and Combining the Masks\n","\n","We have the paths. Now make dictionary of the paths and the hashed images to check for duplicates."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def compute_image_hash(data_dir, path):\n","\n","    image = return_image(data_dir, path)\n","    return hashlib.md5(image.tobytes()).hexdigest()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def image_hash_dict(data_dir, paths):\n","\n","    image_hashes = {}\n","\n","    for path in paths:\n","        image_hashes[path] = compute_image_hash(data_dir, path)\n","\n","    return image_hashes"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Use default dict to see which image appears in multiple paths\n","# Takes dictionary with key: value pairs of image path: image hash\n","# Returns dictionary of hash values (that appear more than once), with list of paths as values\n","def get_image_duplicates(hashes):\n","    hash_path_dict = defaultdict(list)\n","        \n","    for path, image_hash in hashes.items():\n","        hash_path_dict[image_hash].append(path)\n","\n","    duplicates = {k: v for k, v in hash_path_dict.items() if len(v) > 1}\n","    return duplicates\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Function that takes list of path names, and combines the masks of these paths. Clips to 1 to keep mask binary.\n","def combine_masks(data_dir, duplicate_paths):\n","    # all images and masks are (600,800)\n","    combined_mask = np.zeros(shape=(600,800))\n","\n","    # for each path, return the mask and add to combined mask\n","    for path in duplicate_paths:\n","        combined_mask += return_mask(data_dir, path)\n","\n","    # clip the mask\n","    combined_mask = np.clip(combined_mask, 0, 1)\n","    \n","    return combined_mask"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# FUNCTION TO COMBINE DUPLICATE MASKS, SAVE NEW ONE AND DELETE DUPLICATES\n","def deal_with_duplicates(data_dir, paths):\n","\n","    # Get path to hash dictionary\n","    print('getting hash dict...')\n","    hash_dict = image_hash_dict(data_dir, paths)\n","\n","    # Get duplicates\n","    duplicates = get_image_duplicates(hash_dict)\n","    \n","    # For each set of duplicates,\n","    print('looping through duplicates...')\n","    for duplicate_paths in duplicates.values():\n","        \n","        # Combine masks into one\n","        combined_mask = combine_masks(data_dir, duplicate_paths)\n","\n","        # Overwrite first mask\n","        first_path = duplicate_paths[0]\n","        combined_mask_img = Image.fromarray(combined_mask)\n","        \n","        # Save the single-channel image, overwriting the original file\n","        combined_mask_img.save(DATA_DIR + '/' + first_path + '_mask.tif')\n","\n","        # Delete duplicate images and masks\n","        for path in duplicate_paths[1:]:\n","            os.remove(os.path.join(data_dir, path) + '.jpg')\n","            os.remove(os.path.join(data_dir, path) + '_mask.tif')\n","            print(f\"Deleted {path} from the dataset.\")\n","        \n","    print('done.')"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["getting hash dict...\n","looping through duplicates...\n","Deleted validation_dataset/BM/BM53_2 from the dataset.\n","Deleted validation_dataset/BM/BM58_2 from the dataset.\n","Deleted validation_dataset/BM/BM58_7 from the dataset.\n","Deleted validation_dataset/FM/FM79_6 from the dataset.\n","done.\n"]}],"source":["deal_with_duplicates(DATA_DIR, val_paths)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
