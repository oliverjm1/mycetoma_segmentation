{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preprocessing Training Data\n","\n","The Ground Truth masks are a bit problematic, because often only one of multiple grains are included in the ground truth mask.\n","There are some events of the same image being present twice, with two different ground truth masks.\n","This notebook will look at finding these duplicate images and combining the masks into one.\n","\n","## To Do\n","1. Create list of patient ID (duplicate images will always belong to the same patient as eachother).\n","1. Search through images of each patient, checking for duplicates.\n","1. If there is a duplicate, combine the masks, overwrite the mask, and delete the second image/mask occurrence."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1654,"status":"ok","timestamp":1720776143607,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"uOgfpJNngHPy"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import glob\n","import pandas as pd\n","from PIL import Image\n","import cv2\n","import sys\n","\n","sys.path.append('../src')\n","from utils import return_image_and_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":943,"status":"ok","timestamp":1720776292463,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"1HMxckWoOcN9","outputId":"b6c0de01-f80d-4ca3-9459-1e940bded25e"},"outputs":[],"source":["!ls ../data/"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1720776338533,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"2bbLeT3YOY4r"},"outputs":[],"source":["DATA_DIR = '../data'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11894,"status":"ok","timestamp":1720776356840,"user":{"displayName":"Oliver Mills","userId":"17406452132224827755"},"user_tz":-60},"id":"gHPGNzTAH9hb","outputId":"10e59240-050d-4b9f-a243-cdecd21405b8"},"outputs":[],"source":["# Get full image path by adding filename to base path\n","\n","# Get the paths\n","train_paths = np.array([os.path.relpath(i, DATA_DIR).split('.')[0] for i in glob.glob(f'{DATA_DIR}/training_dataset/**/*.jpg')])\n","val_paths = np.array([os.path.relpath(i, DATA_DIR).split('.')[0] for i in glob.glob(f'{DATA_DIR}/validation_dataset/**/*.jpg')])\n","\n","print(f\"Train length: {len(train_paths)}\")\n","print(f\"Val length: {len(val_paths)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort paths to make sorting by patient easier\n","train_paths.sort()\n","val_paths.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["idx = 0\n","print(train_paths[idx:idx+10])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to split path and return only the patient code (e.g. BM10)\n","def get_patient_id(path):\n","    \n","    # First take final part of the path, then take everything before the underscore\n","    patient_id = path.split('/')[2].split('_')[0]\n","    return patient_id"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_patient_id(train_paths[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_patient_ids = list(set([get_patient_id(path) for path in train_paths]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_patient_ids"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define function for plotting image and ground truth side by side when given a path\n","def plot_image_and_mask(path, DATA_DIR = '../data/'):\n","\n","    # Load image and mask\n","    im, mask = return_image_and_mask(DATA_DIR, path)\n","\n","    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","\n","    ax[0].imshow(im)\n","    ax[0].set_title('Image')\n","    ax[0].axis('off')\n","\n","    ax[1].imshow(mask)\n","    ax[1].set_title('Mask')\n","    ax[1].axis('off')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For a given patient, print out all images and masks\n","def get_patient_paths(paths, patient_id):\n","    patient_paths = [path for path in paths if patient_id in path]\n","    return patient_paths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patient_id = train_patient_ids[7]\n","print(f'PATIENT: {patient_id}')\n","patient_paths = get_patient_paths(train_paths, patient_id)\n","for path in patient_paths:\n","    print(path)\n","    plot_image_and_mask(path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to match features of two images\n","# To be used as part of process to check if two images share an overlapping region\n","def detect_and_match_features(image1, image2):\n","    # Convert images to grayscale\n","    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n","    \n","    # Detect ORB features and compute descriptors\n","    orb = cv2.ORB_create()\n","    keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n","    keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n","    \n","    # Match descriptors using BFMatcher\n","    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","    matches = bf.match(descriptors1, descriptors2)\n","    \n","    # Sort matches by distance\n","    matches = sorted(matches, key=lambda x: x.distance)\n","    \n","    return keypoints1, keypoints2, matches"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# After keypoints and matches of two images have been calculated,\n","# this function takes the mean of the top 10 match translations \n","def estimate_translation(keypoints1, keypoints2, matches):\n","    \n","    # Calculate the translation vector\n","    translations = []\n","\n","    for match in matches[:10]:\n","            pt1 = keypoints1[match.queryIdx].pt\n","            pt2 = keypoints2[match.trainIdx].pt\n","            translations.append((pt2[0] - pt1[0], pt2[1] - pt1[1]))\n","\n","    translations = np.array(translations)\n","    median_translation = np.rint(np.median(translations, axis=0))\n","\n","    return median_translation\n","\n","# Function to apply x-y translation to an image\n","def translate_image(image, translation):\n","\n","    rows, cols = image.shape[:2]\n","    translation = (-translation[0], -translation[1])\n","    translation_matrix = np.float32([[1, 0, translation[0]], [0, 1, translation[1]]])\n","    translated_image = cv2.warpAffine(image, translation_matrix, (cols, rows))\n","\n","    return translated_image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Once an image has been translated, find the span of the nonzero part (the overlap part)\n","def find_overlap_bbox(image):\n","\n","    # Find coordinates of all non-zero pixels\n","    coords = cv2.findNonZero(image[...,0])\n","    \n","    # Calculate the bounding box of the non-zero region\n","    x, y, w, h = cv2.boundingRect(coords)\n","    min_x, min_y, max_x, max_y = x, y, x + w, y + h\n","\n","    return min_x, min_y, max_x, max_y"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## VISUALISATION FUNCTIONS\n","\n","# Plot to show matched features between two images\n","def visualize_matches(image1, image2, keypoints1, keypoints2, matches):\n","    matched_image = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","    plt.figure(figsize=(15, 10))\n","    plt.title('Matched Features')\n","    plt.imshow(cv2.cvtColor(matched_image, cv2.COLOR_BGR2RGB))\n","    plt.show()\n","\n","# Plot to show images before and after translation\n","def visualize_translation(image1, image2, translated_image2):\n","    plt.figure(figsize=(15, 5))\n","    \n","    plt.subplot(1, 3, 1)\n","    plt.title('Original Image 1')\n","    plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB))\n","    \n","    plt.subplot(1, 3, 2)\n","    plt.title('Original Image 2')\n","    plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n","    \n","    plt.subplot(1, 3, 3)\n","    plt.title('Translated Image 2')\n","    plt.imshow(cv2.cvtColor(translated_image2, cv2.COLOR_BGR2RGB))\n","    \n","    plt.show()\n","\n","# Plot to show the difference between the first image and the translated second image\n","def visualize_difference(image1, translated_image2):\n","    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","    gray2 = cv2.cvtColor(translated_image2, cv2.COLOR_BGR2GRAY)\n","    difference = cv2.absdiff(gray1, gray2)\n","    \n","    plt.figure(figsize=(10, 5))\n","    plt.title('Difference Image')\n","    plt.imshow(difference, cmap='hot')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def check_overlap(image1, image2, visualize=True):\n","    # Detect and match features\n","    keypoints1, keypoints2, matches = detect_and_match_features(image1, image2)\n","    \n","    # Estimate translation\n","    translation = estimate_translation(keypoints1, keypoints2, matches)\n","    \n","    # Translate the second image\n","    translated_image2 = translate_image(image2, translation)\n","    \n","    # Convert images to grayscale\n","    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","    gray2 = cv2.cvtColor(translated_image2, cv2.COLOR_BGR2GRAY)\n","\n","    # Crop both to matching region\n","    xl,yl,xh,yh = find_overlap_bbox(translated_image2)\n","    crop1 = gray1[yl:yh,xl:xh]\n","    crop2 = gray2[yl:yh,xl:xh]\n","\n","    # Gaussian Blur before measuring difference to reduce alignment error\n","    blurred_crop1 = cv2.GaussianBlur(crop1, (11, 11), 0)\n","    blurred_crop2 = cv2.GaussianBlur(crop2, (11, 11), 0)\n","    \n","    # Compute absolute difference between images\n","    diff = cv2.absdiff(blurred_crop1, blurred_crop2)\n","\n","    # Threshold the difference image\n","    threshold_val = 40\n","    _, thresh = cv2.threshold(diff, threshold_val, 255, cv2.THRESH_BINARY)\n","\n","    if visualize:\n","        # Visualize matched features\n","        visualize_matches(image1, image2, keypoints1, keypoints2, matches)\n","\n","        # Visualize translation result\n","        visualize_translation(image1, image2, translated_image2)\n","\n","        # Visualise difference between matching areas\n","        plt.title('Difference')\n","        plt.imshow(diff, cmap='hot')\n","        plt.show()\n","\n","        # And when threshold is applied\n","        plt.title(f'thresholded at {threshold_val}')\n","        plt.imshow(thresh)\n","        plt.show()\n","\n","    # Calculate percentage of overlap\n","    overlap = np.count_nonzero(thresh == 0) / np.size(thresh)\n","\n","    return overlap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["patient = train_patient_ids[0]\n","paths = get_patient_paths(train_paths, patient)\n","\n","# Load your images\n","image1, _ = return_image_and_mask(DATA_DIR, paths[1])\n","image2, _ = return_image_and_mask(DATA_DIR, paths[2])\n","\n","check_overlap(image1, image2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's check if this works:\n","\n","match_count = 0\n","\n","# cycle through patients\n","for patient in train_patient_ids:\n","    \n","    patient_paths = get_patient_paths(train_paths, patient)\n","\n","    # cycle through paths\n","    for path1 in patient_paths:\n","        image1, mask1 = return_image_and_mask(DATA_DIR, path1)\n","\n","        # Check all paths in list\n","        for path2 in patient_paths[:10]:\n","            if path1 == path2:\n","                continue\n","            \n","            image2, mask2 = return_image_and_mask(DATA_DIR, path2)\n","\n","            if (image1==image2).all():\n","                continue\n","            \n","            overlap = check_overlap(image1, image2, visualize=False)\n","            #print(f'{path1} and {path2} have Overlap percentage: {overlap * 100:.2f}%')\n","            if overlap > 0.9:\n","                print(\"We Got a Match !! :)\")\n","                print(f'Between {path1} and {path2}')\n","\n","                fig, ax = plt.subplots(2, 2, figsize=(10, 5))\n","\n","                ax[0][0].imshow(image1)\n","                ax[0][0].set_title('Image1')\n","                ax[0][0].axis('off')\n","\n","                ax[0][1].imshow(image2)\n","                ax[0][1].set_title('Image2')\n","                ax[0][1].axis('off')\n","\n","                ax[1][0].imshow(mask1)\n","                ax[1][0].set_title('Mask1')\n","                ax[1][0].axis('off')\n","\n","                ax[1][1].imshow(mask2)\n","                ax[1][1].set_title('Mask2')\n","                ax[1][1].axis('off')\n","\n","                plt.show()\n","\n","                match_count += 0.5 # matching both ways at the moment\n","                print(f'Matches so far: {np.ceil(match_count)}')\n","\n","            \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
